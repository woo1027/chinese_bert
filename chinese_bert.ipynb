{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMX/miCzgTOWQL7Jy1+12J6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woo1027/chinese_bert/blob/main/chinese_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T--kSUgWJhLd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        " function ClickConnect(){\n",
        "   btn = document.querySelector(\"colab-connect-button\")\n",
        "   if (btn != null){\n",
        "     console.log(\"Click colab-connect-button\");\n",
        "     btn.click()\n",
        "     }\n",
        "\n",
        "   btn = document.getElementById('ok')\n",
        "   if (btn != null){\n",
        "     console.log(\"Click reconnect\");\n",
        "     btn.click()\n",
        "     }\n",
        "  }\n",
        "\n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "\n",
        "print(\"Done.\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "z9nGu1_cJ4jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toxic = pd.read_csv(\"/content/drive/My Drive//toxic.csv\")\n",
        "toxic.head()"
      ],
      "metadata": {
        "id": "-OsZfuXOJyAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"訓練集遺失值：\\n{}\".format(toxic.isnull().sum()))"
      ],
      "metadata": {
        "id": "5buKAbi3J9Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toxic = toxic.dropna(axis=0)\n"
      ],
      "metadata": {
        "id": "cRmiOFWnJyBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdata = toxic.drop(columns=['Unnamed: 0'])\n",
        "newdata"
      ],
      "metadata": {
        "id": "K7oBABvHKt5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean_com_chinese欄位 為 翻譯成中文的評論"
      ],
      "metadata": {
        "id": "jizVuDeJKtgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_stats = newdata[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum()\n",
        "print(label_stats)\n",
        "\n",
        "comments=newdata['clean_com_chinese'].to_list()\n",
        "comments[:5]"
      ],
      "metadata": {
        "id": "mr-y5z72K9EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "j71oArxVK9DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training, testing sets & validation sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    comments, newdata.iloc[:,2:8].values, test_size=0.3, random_state=1)\n",
        "\n",
        "#validation set\n",
        "test_data, val_data, test_labels, val_labels = train_test_split(\n",
        "    test_data, test_labels, test_size=0.67, random_state=1)\n",
        "\n",
        "print('numbers of training Dataset ',len(train_data))\n",
        "print('numbers of testing Dataset',len(test_data))\n",
        "print('numbers of validation Dataset',len(val_data))"
      ],
      "metadata": {
        "id": "PSLUJapmK892"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_encode(tokenizer, comments, labels, max_length=128):\n",
        "    # Initialize empty lists to store tokenized inputs and attention masks\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # Iterate through each comment in the 'comments' list\n",
        "    for comment in comments:\n",
        "        # Tokenize and encode the comment using the BERT tokenizer\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            comment,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Append the tokenized input and attention mask to their respective lists\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    # Convert the lists of tokenized inputs and attention masks to PyTorch tensors\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    # Convert the labels to a PyTorch tensor with the data type float32\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    # Return the tokenized inputs, attention masks, and labels as PyTorch tensors\n",
        "    return input_ids, attention_masks, labels"
      ],
      "metadata": {
        "id": "vHPTbqK9K885"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "b73kv40zLZhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "model =model.to(device)"
      ],
      "metadata": {
        "id": "izdzqrpgK8xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Token Initialization\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese', do_lower_case=True)\n",
        "\n",
        "# Model Initialization\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=6)"
      ],
      "metadata": {
        "id": "kQsDRfTTJyGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and Encode the comments and labels for the training set\n",
        "input_ids, attention_masks, labels = tokenize_and_encode(\n",
        "    tokenizer,\n",
        "    train_data,\n",
        "    train_labels\n",
        ")\n",
        "\n",
        "# Step 4: Tokenize and Encode the comments and labels for the test set\n",
        "test_input_ids, test_attention_masks, test_labels = tokenize_and_encode(\n",
        "    tokenizer,\n",
        "    test_data,\n",
        "    test_labels\n",
        ")\n",
        "\n",
        "# Tokenize and Encode the comments and labels for the validation set\n",
        "val_input_ids, val_attention_masks, val_labels = tokenize_and_encode(\n",
        "    tokenizer,\n",
        "    val_data,\n",
        "    val_labels\n",
        ")"
      ],
      "metadata": {
        "id": "O4TAGq5SLhRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating DataLoader for the balanced dataset\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#test\n",
        "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#val\n",
        "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "JElzumUdQygC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ljyEMoZ8Ncdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and model from the saved directory\n",
        "model_name =\"/content/drive/My Drive/Saved_model\"\n",
        "Bert_Tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "Bert_Model = BertForSequenceClassification.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "s8YMTUPHNcwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_user_input(input_text, model=Bert_Model, tokenizer=Bert_Tokenizer,device=device):\n",
        "    user_input = [input_text]\n",
        "\n",
        "    user_encodings = tokenizer(user_input, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    user_dataset = TensorDataset(user_encodings['input_ids'], user_encodings['attention_mask'])\n",
        "\n",
        "    user_loader = DataLoader(user_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in user_loader:\n",
        "            input_ids, attention_mask = [t.to(device) for t in batch]\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.sigmoid(logits)\n",
        "    predicted_labels = (predictions.cpu().numpy() > 0.5).astype(int)\n",
        "    return predicted_labels[0].tolist()"
      ],
      "metadata": {
        "id": "M9F3yG9ONi0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '你是智障嗎?'\n",
        "predict_user_input(input_text=text)"
      ],
      "metadata": {
        "id": "8EILhRgfNcxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "有毒的、嚴重有毒、猥褻、威脅、侮辱、身分仇恨"
      ],
      "metadata": {
        "id": "IcG7xaZjRZi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"你真的很討厭，但我愛你\"\n",
        "predict_user_input(model=Bert_Model,\n",
        "                   tokenizer=Bert_Tokenizer,\n",
        "                   input_text=text,\n",
        "                   device=device)"
      ],
      "metadata": {
        "id": "1-_tvVJdRYQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "有毒的、嚴重有毒、猥褻、威脅、侮辱、身分仇恨"
      ],
      "metadata": {
        "id": "DPeNF1oxU6AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = '你這個婊子'\n",
        "predict_user_input(input_text)"
      ],
      "metadata": {
        "id": "oB6F2pX-Ru61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "有毒的、嚴重有毒、猥褻、威脅、侮辱、身分仇恨"
      ],
      "metadata": {
        "id": "ny21xcKuVBoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"閉嘴，笨蛋\"\n",
        "predict_user_input(model=Bert_Model,\n",
        "                   tokenizer=Bert_Tokenizer,\n",
        "                   input_text=text,\n",
        "                   device=device)"
      ],
      "metadata": {
        "id": "q2fkavxqRspd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "有毒的、嚴重有毒、猥褻、威脅、侮辱、身分仇恨"
      ],
      "metadata": {
        "id": "Qga9coXyVCei"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-XCjR6dqR1b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lRv9xsXdVKMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "23PN47HIVP8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bK99fAFzVQY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "38IYZ2q0VTsV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}